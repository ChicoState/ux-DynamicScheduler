# Phase III: Prototypes and User Testing

## Introduction

The goal of Dynamic Scheduler is to provide users with a simple way to schedule their tasks with their planned events. A key component on the software side is to allow the application to automatically assign a task to a convenient date/time. For this phase of the project, we took bigger steps towards in-person user-testing. Leading up to the tests, we prepared ourselves by earning a CITI certificate that would allow us to perform usability-testing as well as completing the prototype and a few forms for verification.

## Methods

The Dynamic Scheduler Usability Engineering team conducted pilot tests with six participants on December 10, 2024. Prior to the user testing, the team worked collaboratively to prepare a prototype of the Dynamic Scheduler application, finalize the IRB documentation, draft an Informed Consent Form, and develop a detailed Protocol/Script for the study. The Protocol defined the study’s structure, detailing the tasks participants would be asked to perform and the data collection methods used to assess usability and functionality. During the planning phase, the UX team determined the primary focus of this study would be addressing the efficiency and usability of the Dynamic Scheduler application in comparison to a competitor’s application.

The tasks for the participants were designed to evaluate key components of the application’s usability and functionality.
* Task 1 required participants to create and save an event titled “First Dinner of the Year” scheduled for January 1, 2025, at 5:45 PM. This task was designed to test the efficiency and usability of the application's most basic functionality as a calendar. It focused on whether users could easily navigate through the interface to add events or tasks to a specific date, emphasizing the importance of this basic, yet essential feature being straightforward to locate and use.

* Task 2 asked participants to edit an existing task, “Go to club meeting,” to repeat weekly and rename it to “Go to coding club meeting.” This task was intended to evaluate the application’s ability to allow users to modify and manage their schedules effectively. Editing and deleting events are fundamental features of any digital calendar, and this task ensured the application provided users with appropriate control over their schedules.

* Task 3 involved creating an account using the email “testuser@gmail.com” and password “pass” through the Create Account process. Since the Login screen served as the application’s homepage, this task aimed to assess whether the login and account creation feature was intuitive and user friendly. This would ensure users could easily start saving data to their specific accounts. A successful login would direct the user to a calendar view displaying the current month as a grid, with each day clearly organized.

Our approach to the study was formative, aimed at identifying usability issues and gathering insights to improve the application before its final release. We conducted six 30-minute user tests, focusing on how participants interacted with the application and identifying areas for improvement. To achieve this, we applied a “think-aloud” method, encouraging participants to vocalize their actions, thoughts, likes, dislikes, and any moments of surprise or confusion as they navigated the application. This approach provided valuable real-time feedback on the user experience. It enabled the UX team to understand the participant’s perspectives and document any challenges participants encountered. This would provide data that could later be used to improve the Dynamic Scheduler application and eliminate those issues. The only requirement for participation in our Pilot Test was that participants needed to be over the age of 18.

Before the Pilot Test began, participants were read the Informed Consent Form and asked to sign it to confirm their understanding of the study including potential risks and their willingness to participate. This is significant because consent is an ongoing activity. Even after consent has initially been given, the participant reserves the right to withdraw consent and remove themselves from the study entirely. The session began with an introduction, followed by background questions designed to gather information about the participants' prior experience with similar applications. Once the responses were recorded, the Facilitator proceeded with the tasks. The primary objective of the three tasks were to evaluate the efficiency of the Dynamic Scheduler application in comparison to the competitor’s application.

Moderators were responsible for collecting and documenting data throughout the Pilot Test. After participants completed the tasks, they were asked additional questions to debrief about their experience with the Dynamic Scheduler application, intended to gather qualitative feedback. These questions focused on understanding user impressions, such as what they liked best, areas they felt could be improved, whether the app’s organization made sense, and if they found it intuitive to use.

## Findings

Overall, all of our participants were able to successfully complete each of the three tasks. While this reflects well on our implementation, there was still notable feedback on how to make each process easier to accomplish. When asking for a rating on a scale from 1 to 5 (1 being very difficult and 5 being very easy), there was one outlier while the rest received a high rating.
* Task 1 yielded an average difficulty score of 3.5, rated on a scale from 1 to 5, 1 being very difficult and 5 being very easy. Some of the feedback that we got from Task 1 that explains the score is that some participants had little trouble finding the “Add Event” button.

* Task 2 was completed fairly easily: yielding an average difficulty score of 4.6. Participant's comments on the task were that it was “pretty simple.”

* Task 3 was completed very easily which yielded an average difficulty score of 5. Participant's comments on the task were that it was “super easy.” All participants were able to find the task, “Go to club meeting,” change it to “Go to coding club meeting” and toggle the repeat feature to have the task occur weekly.

In our debrief with participants when we asked them “what did you like best,” the most consistent response was that the app was very simple to use and that the layout was clear. When asked “What could be improved,” the most consistent response from participants was to put an “Add Event” button on the main page, so that users would be able to add an event much quicker and more directly. When asked “Does the organization of the app make sense,” the majority of participant responses were that the organization of the app made sense. When we asked participants “How intuitive did you find the app overall,” the majority of the responses were that they found the app pretty intuitive.

* A link to the spreadsheet for data collection, with rows filled out representing the data collected: https://docs.google.com/spreadsheets/d/1PSCikeCIrhk1k8HZbhtQ448kz3sxP0p5FHFNzU0e6cI/edit?gid=0#gid=0 

## Conclusions

Based on our findings the main thing we were recommended to add was an “Add Event” button on the main page. This is because users had a bit of trouble finding the “Add Event” button on the hamburger menu, with one user commenting on wanting “immediate actions like 'Add Event' not hidden behind a menu”, This feedback makes sense as users want to immediately add an event when they get on our app, but with our current design users have to through the menu page to get to the “Add Event” page. Adding an “Add Event” button on the main page would fix this issue. The aspects of design that were affirmed to remain as is by participants were the organization or simplicity of the app as users found the app very intuitive. Another thing affirmed to remain as is was the layout of the “Add Event” page as participants easily navigated through this page once it was found.

## Caveats

One caveat of this sprint was the participant assignment process. The participants signed up for our group voluntarily, and since they were our classmates (some of which were also our friends), this could have introduced bias or affected the objectivity of the study. Another caveat was our inexperience in conducting Pilot Tests which influenced how we structured our questions and recorded data. However, this experience gave us valuable insight and ideas for improving our method in future studies.
